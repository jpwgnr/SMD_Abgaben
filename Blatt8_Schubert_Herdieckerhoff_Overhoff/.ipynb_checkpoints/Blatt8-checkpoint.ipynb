{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blatt 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was beschreibt die Lossfunktion?\n",
    "\n",
    "Wir betrachten zwei Wahrscheinlichkeitsdichtefunktionen $p$ und $q$. Die Lossfunktion ergibt kleinere Werte für ähnliche Verteilungen $q$ und $p$. Bei einem Klassifizierungsproblem ist $p$ die wahre Verteilung der Klassen für ein gegebenes $x$, $q(x)$ ist die geschätzte Verteilung, die vom Machine-Learner erstellt wurde. Die Loss-Funktion gibt praktisch die Abweichung der geschätzen Verteilung von der wahren Verteilung an, die pro Entscheidung entsteht. Diese Abweichung soll in Klassifikationsproblemen minimiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie kann sie minimiert werden?\n",
    "\n",
    "Die Gewichtsmatrix $\\mathbf{W}$ wird minimiert. Es kann z.B. zufällig nach Matrixeinträgen gesucht werden, die eine bessere Loss-Funktion erhalten. Es geht aber auch anders. \n",
    "Oft genügt es ein lokales Optimum zu finden. Das Problem kann auch konvex umformuliert werden. Dann findet der lokale Optimierer garantiert das globale Optimum.\n",
    "Man kann es mit einem inkrementellen Ansatz verschen. Dabei behalten wir das beste gefundene $\\mathbf{W}$ und fügen eine zufällige Richtung hinzu. Noch besser wird man, wenn man in jeder Iteration den Gradienten der Verlustfunktion verfolgt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welche Funktion haben Aktivierungsfunktionen bzw. welche Probleme werden durch sie gelöst? \n",
    "\n",
    "\n",
    "Aktivierungsfunktion soll die Aktivierung eines biologischen Neurons simulieren. Sie bestimmt die Ausgabe des künstlichen Neurons. Als Aktivierungsfunktion können nichtlineare Funktionen verwendet werden, die den Raum verzerren und somit auch linear nicht lösbare Probleme lösbar machen, ohne die Komplexität des Netzes zu erhöhen.\n",
    "\n",
    "Gängige Aktivierungsfunktionen sind die Sigmoid-Funktion, der Tangens Hyperbolicus uns ReLu (Rectified Linear Unit).\n",
    "\n",
    "Sie werden benutzt um in der End-/Ausgabeschicht eines Nezwerks Wahrscheinlichkeitsschätzungen anzustellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was ist ein Neuron?\n",
    "\n",
    "Ein Neuron ist das Basisobjekt für neuronale Netze. Es kann Eingaben verarbeiten und reagieren. Ein Neuron kann durch vier Basiselemente beschrieben werden:\n",
    "* Gewichtung: Die Gewichte bestimmen den Grad ds Einflusses den die Eingaben des Neurons in der Berechnung einnimmt.\n",
    "* Übertragungsfunktion: Berechnet Anahnd der Gewichtung der Eingabe die Netzeingabe.\n",
    "* Aktivierungsfunktion: Bestimmt die Ausgabe. Wird beeinflusst durch die Netzeingabe und einem Schwellwert.\n",
    "* Schwellwert: Addieren eines Schwellenwerts verschiebt die gewichtete Eingabe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier drei Anwendungsbeispiele für Neuronale Netze.\n",
    "\n",
    "* Gesichtserkennung\n",
    "* Spracherkennung\n",
    "* Autonomes Fahren\n",
    "\n",
    "Geberell sind neuronale Netze immer besonders sinnvoll, wenn wenig a priori Wissen über das Problem vorliegt und eine große Datenmenge verarbeitet werden muss, um wenige Ergebnisse zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "Dimensionen:\n",
    "* $x_i = $ \n",
    "* $C = $\n",
    "* $W = $\n",
    "* $b = $\n",
    "* $\\nabla_W \\hat{C} = $\n",
    "* $\\nabla_{f_i} \\hat{C} = $\n",
    "* $\\frac{\\partial f_{k,i}}{\\partial W} = $\n",
    "* $\\frac{\\partial f_{k,i}}{\\partial b} = $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
